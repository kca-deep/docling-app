[Unit]
Description=EXAONE-Deep 7.8B LLM Server - RAG Mode (ctx=8K p=1)
Documentation=https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B-GGUF
After=network.target

[Service]
Type=simple
User=kca
Group=kca
WorkingDirectory=/models/exaone-deep

Environment="CUDA_VISIBLE_DEVICES=0"
Environment="LD_LIBRARY_PATH=/models/llama.cpp/build/bin"

# RAG Mode: ctx=8K, parallel=1, n-predict=8K
ExecStart=/models/llama.cpp/build/bin/llama-server \
    -m /models/exaone-deep/EXAONE-Deep-7.8B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8085 \
    -c 8192 \
    -np 1 \
    -n 8192 \
    -ngl 99 \
    --jinja \
    --chat-template-file /models/exaone-deep/chat_template.jinja \
    --temp 0.6 \
    --top-p 0.95 \
    --no-warmup

Restart=on-failure
RestartSec=10
TimeoutStartSec=120
TimeoutStopSec=30

LimitNOFILE=65536
LimitMEMLOCK=infinity

StandardOutput=journal
StandardError=journal
SyslogIdentifier=exaone-deep-rag

[Install]
WantedBy=multi-user.target
