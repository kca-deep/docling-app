# /etc/systemd/system/llama-server-qwen3vl-rag.service
# RAG Mode: ctx=16K, p=2, n=8K
# v2: KV cache quantization (q8_0) for VRAM reduction
[Unit]
Description=Llama.cpp Server - Qwen3-VL 8B (RAG Mode, ctx=16K p=2 n=8K, KV q8_0)
After=network.target
Wants=network-online.target

[Service]
Type=simple
User=kca
WorkingDirectory=/models/llama.cpp
Environment="CUDA_VISIBLE_DEVICES=0"
ExecStart=/models/llama.cpp/build/bin/llama-server \
  -m /models/qwen3-vl-gguf/Qwen3VL-8B-Instruct-Q4_K_M.gguf \
  --mmproj /models/qwen3-vl-gguf/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf \
  --host 0.0.0.0 \
  --port 8084 \
  --ctx-size 16384 \
  --parallel 2 \
  --cont-batching \
  --defrag-thold 0.1 \
  --jinja \
  -fa 1 \
  -ngl 99 \
  --cache-type-k q8_0 \
  --cache-type-v q8_0 \
  --temp 0.7 \
  --top-p 0.9 \
  --repeat-penalty 1.1 \
  --n-predict 8192 \
  --threads 8 \
  --batch-size 4096 \
  --ubatch-size 1024
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-server-qwen3vl-rag
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
